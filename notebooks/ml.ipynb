{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522da4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing SentimentModel...\n",
      "Model initialized. Parameters: 66,365,187\n",
      "\n",
      "=== FORWARD PASS TESTS ===\n",
      "\n",
      "1. Single text forward pass:\n",
      "Input: 'This product is absolutely amazing!'\n",
      "Logits shape: torch.Size([1, 3])\n",
      "Logits: tensor([[-0.2018,  0.2261,  0.3786]])\n",
      "\n",
      "2. Batch forward pass:\n",
      "Batch size: 3\n",
      "Logits shape: torch.Size([3, 3])\n",
      "Batch logits:\n",
      "tensor([[-0.3371,  0.1203,  0.3568],\n",
      "        [-0.4509,  0.1856,  0.3680],\n",
      "        [-0.3447,  0.2293,  0.4077]])\n",
      "\n",
      "3. Prediction methods:\n",
      "Probabilities:\n",
      "tensor([[0.2183, 0.3449, 0.4369],\n",
      "        [0.1939, 0.3664, 0.4397],\n",
      "        [0.2042, 0.3625, 0.4333]])\n",
      "Predictions: [2 2 2]\n",
      "Confidence scores: [0.43687063 0.4397237  0.43328735]\n",
      "\n",
      "=== BACKWARD PASS TEST ===\n",
      "\n",
      "4. Backward pass:\n",
      "Loss: 1.1232\n",
      "Gradients computed: True\n",
      "\n",
      "Sample gradient norms:\n",
      "  distilbert.embeddings.word_embeddings.weight: 0.436941\n",
      "  distilbert.embeddings.position_embeddings.weight: 0.440324\n",
      "  distilbert.embeddings.LayerNorm.weight: 0.026794\n",
      "  distilbert.embeddings.LayerNorm.bias: 0.044529\n",
      "  distilbert.transformer.layer.0.attention.q_lin.weight: 0.082341\n",
      "\n",
      "✅ Model test completed successfully!\n",
      "✅ Forward pass: Working\n",
      "✅ Backward pass: Working\n",
      "✅ Ready for training!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')  \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ml.models import SentimentModel\n",
    "\n",
    "def test_model():\n",
    "    print(\"Testing SentimentModel...\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = SentimentModel()\n",
    "    print(f\"Model initialized. Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Test data\n",
    "    single_text = \"This product is absolutely amazing!\"\n",
    "    batch_texts = [\n",
    "        \"Great quality, love it!\",\n",
    "        \"Not worth the money, poor quality.\",\n",
    "        \"It's okay, nothing special.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n=== FORWARD PASS TESTS ===\")\n",
    "    \n",
    "    # Test 1: Single text forward pass\n",
    "    print(\"\\n1. Single text forward pass:\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model.forward(single_text)\n",
    "        print(f\"Input: '{single_text}'\")\n",
    "        print(f\"Logits shape: {logits.shape}\")\n",
    "        print(f\"Logits: {logits}\")\n",
    "        \n",
    "    # Test 2: Batch forward pass\n",
    "    print(\"\\n2. Batch forward pass:\")\n",
    "    with torch.no_grad():\n",
    "        batch_logits = model.forward(batch_texts)\n",
    "        print(f\"Batch size: {len(batch_texts)}\")\n",
    "        print(f\"Logits shape: {batch_logits.shape}\")\n",
    "        print(f\"Batch logits:\\n{batch_logits}\")\n",
    "    \n",
    "    # Test 3: Prediction methods\n",
    "    print(\"\\n3. Prediction methods:\")\n",
    "    predictions = model.predict(batch_texts)\n",
    "    print(f\"Probabilities:\\n{predictions}\")\n",
    "    \n",
    "    pred_with_conf = model.predict_with_confidence(batch_texts)\n",
    "    print(f\"Predictions: {pred_with_conf['predictions']}\")\n",
    "    print(f\"Confidence scores: {pred_with_conf['confidence']}\")\n",
    "    \n",
    "    print(\"\\n=== BACKWARD PASS TEST ===\")\n",
    "    \n",
    "    # Test 4: Backward pass (gradient computation)\n",
    "    model.train()\n",
    "    \n",
    "    # Create dummy targets\n",
    "    targets = torch.tensor([2, 0, 1])  # positive, negative, neutral\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = model.forward(batch_texts)\n",
    "    \n",
    "    # Compute loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(logits, targets)\n",
    "    \n",
    "    print(f\"\\n4. Backward pass:\")\n",
    "    print(f\"Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Check if gradients were computed\n",
    "    has_gradients = any(p.grad is not None for p in model.parameters())\n",
    "    print(f\"Gradients computed: {has_gradients}\")\n",
    "    \n",
    "    # Print a few gradient norms to verify\n",
    "    grad_norms = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            grad_norms.append((name, grad_norm))\n",
    "    \n",
    "    print(f\"\\nSample gradient norms:\")\n",
    "    for name, norm in grad_norms[:5]:  # Show first 5\n",
    "        print(f\"  {name}: {norm:.6f}\")\n",
    "    \n",
    "    print(f\"\\n✅ Model test completed successfully!\")\n",
    "    print(f\"✅ Forward pass: Working\")\n",
    "    print(f\"✅ Backward pass: Working\") \n",
    "    print(f\"✅ Ready for training!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6547183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../ml/models/production_sentiment_model.pth\n",
      "Text: This foundation is absolutely perfect for my skin tone!\n",
      "Prediction: positive (confidence: 1.000)\n",
      "Probabilities: neg=0.000, neu=0.000, pos=1.000\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Worst mascara ever. Clumpy and flakes off within an hour.\n",
      "Prediction: negative (confidence: 0.999)\n",
      "Probabilities: neg=0.999, neu=0.001, pos=0.001\n",
      "--------------------------------------------------------------------------------\n",
      "Text: It's okay, nothing special but does the job I guess.\n",
      "Prediction: neutral (confidence: 0.803)\n",
      "Probabilities: neg=0.110, neu=0.803, pos=0.087\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Love love love this lipstick! The color is gorgeous and it lasts all day.\n",
      "Prediction: positive (confidence: 1.000)\n",
      "Probabilities: neg=0.000, neu=0.000, pos=1.000\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Complete waste of money. Broke after one use.\n",
      "Prediction: negative (confidence: 0.999)\n",
      "Probabilities: neg=0.999, neu=0.001, pos=0.001\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Pretty good quality for the price. Would recommend.\n",
      "Prediction: positive (confidence: 0.979)\n",
      "Probabilities: neg=0.001, neu=0.020, pos=0.979\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Meh. Expected better based on the reviews.\n",
      "Prediction: negative (confidence: 0.710)\n",
      "Probabilities: neg=0.710, neu=0.283, pos=0.007\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Holy grail product! Can't live without it now.\n",
      "Prediction: positive (confidence: 0.999)\n",
      "Probabilities: neg=0.001, neu=0.000, pos=0.999\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Terrible customer service and the product arrived damaged.\n",
      "Prediction: negative (confidence: 0.995)\n",
      "Probabilities: neg=0.995, neu=0.003, pos=0.002\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Works as expected. Nothing to complain about.\n",
      "Prediction: positive (confidence: 0.958)\n",
      "Probabilities: neg=0.001, neu=0.041, pos=0.958\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Good color but terrible packaging\n",
      "Prediction: negative (confidence: 0.689)\n",
      "Probabilities: neg=0.689, neu=0.238, pos=0.073\n",
      "--------------------------------------------------------------------------------\n",
      "Text: I guess it's fine\n",
      "Prediction: neutral (confidence: 0.576)\n",
      "Probabilities: neg=0.140, neu=0.576, pos=0.284\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Could be better\n",
      "Prediction: neutral (confidence: 0.485)\n",
      "Probabilities: neg=0.232, neu=0.485, pos=0.284\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Not bad\n",
      "Prediction: neutral (confidence: 0.665)\n",
      "Probabilities: neg=0.102, neu=0.665, pos=0.233\n",
      "--------------------------------------------------------------------------------\n",
      "Text: Product product product\n",
      "Prediction: positive (confidence: 0.882)\n",
      "Probabilities: neg=0.011, neu=0.107, pos=0.882\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')  \n",
    "\n",
    "from ml.inference import SentimentPredictor\n",
    "\n",
    "predictor = SentimentPredictor(model_path=\"../ml/models/production_sentiment_model.pth\")  # Uses new trained model\n",
    "\n",
    "# Beauty product reviews (your training domain)\n",
    "test_cases = [\n",
    "    \"This foundation is absolutely perfect for my skin tone!\",\n",
    "    \"Worst mascara ever. Clumpy and flakes off within an hour.\",\n",
    "    \"It's okay, nothing special but does the job I guess.\",\n",
    "    \"Love love love this lipstick! The color is gorgeous and it lasts all day.\",\n",
    "    \"Complete waste of money. Broke after one use.\",\n",
    "    \"Pretty good quality for the price. Would recommend.\",\n",
    "    \"Meh. Expected better based on the reviews.\",\n",
    "    \"Holy grail product! Can't live without it now.\",\n",
    "    \"Terrible customer service and the product arrived damaged.\",\n",
    "    \"Works as expected. Nothing to complain about.\",\n",
    "    \"Good color but terrible packaging\",  # Mixed sentiment\n",
    "    \"I guess it's fine\",  # Weak positive\n",
    "    \"Could be better\",  # Ambiguous\n",
    "    \"Not bad\",  # Double negative\n",
    "    \"Product product product\"  # Nonsense\n",
    "]\n",
    "\n",
    "for text in test_cases:\n",
    "    result = predictor.predict(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Prediction: {result['prediction']['label']} (confidence: {result['prediction']['confidence']:.3f})\")\n",
    "    print(f\"Probabilities: neg={result['probabilities']['negative']:.3f}, neu={result['probabilities']['neutral']:.3f}, pos={result['probabilities']['positive']:.3f}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
