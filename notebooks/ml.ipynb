{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "522da4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing SentimentModel...\n",
      "Model initialized. Parameters: 66,365,187\n",
      "\n",
      "=== FORWARD PASS TESTS ===\n",
      "\n",
      "1. Single text forward pass:\n",
      "Input: 'This product is absolutely amazing!'\n",
      "Logits shape: torch.Size([1, 3])\n",
      "Logits: tensor([[-0.2018,  0.2261,  0.3786]])\n",
      "\n",
      "2. Batch forward pass:\n",
      "Batch size: 3\n",
      "Logits shape: torch.Size([3, 3])\n",
      "Batch logits:\n",
      "tensor([[-0.3371,  0.1203,  0.3568],\n",
      "        [-0.4509,  0.1856,  0.3680],\n",
      "        [-0.3447,  0.2293,  0.4077]])\n",
      "\n",
      "3. Prediction methods:\n",
      "Probabilities:\n",
      "tensor([[0.2183, 0.3449, 0.4369],\n",
      "        [0.1939, 0.3664, 0.4397],\n",
      "        [0.2042, 0.3625, 0.4333]])\n",
      "Predictions: [2 2 2]\n",
      "Confidence scores: [0.43687063 0.4397237  0.43328735]\n",
      "\n",
      "=== BACKWARD PASS TEST ===\n",
      "\n",
      "4. Backward pass:\n",
      "Loss: 1.1232\n",
      "Gradients computed: True\n",
      "\n",
      "Sample gradient norms:\n",
      "  distilbert.embeddings.word_embeddings.weight: 0.436941\n",
      "  distilbert.embeddings.position_embeddings.weight: 0.440324\n",
      "  distilbert.embeddings.LayerNorm.weight: 0.026794\n",
      "  distilbert.embeddings.LayerNorm.bias: 0.044529\n",
      "  distilbert.transformer.layer.0.attention.q_lin.weight: 0.082341\n",
      "\n",
      "✅ Model test completed successfully!\n",
      "✅ Forward pass: Working\n",
      "✅ Backward pass: Working\n",
      "✅ Ready for training!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')  \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ml.models import SentimentModel\n",
    "\n",
    "def test_model():\n",
    "    print(\"Testing SentimentModel...\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = SentimentModel()\n",
    "    print(f\"Model initialized. Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Test data\n",
    "    single_text = \"This product is absolutely amazing!\"\n",
    "    batch_texts = [\n",
    "        \"Great quality, love it!\",\n",
    "        \"Not worth the money, poor quality.\",\n",
    "        \"It's okay, nothing special.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n=== FORWARD PASS TESTS ===\")\n",
    "    \n",
    "    # Test 1: Single text forward pass\n",
    "    print(\"\\n1. Single text forward pass:\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model.forward(single_text)\n",
    "        print(f\"Input: '{single_text}'\")\n",
    "        print(f\"Logits shape: {logits.shape}\")\n",
    "        print(f\"Logits: {logits}\")\n",
    "        \n",
    "    # Test 2: Batch forward pass\n",
    "    print(\"\\n2. Batch forward pass:\")\n",
    "    with torch.no_grad():\n",
    "        batch_logits = model.forward(batch_texts)\n",
    "        print(f\"Batch size: {len(batch_texts)}\")\n",
    "        print(f\"Logits shape: {batch_logits.shape}\")\n",
    "        print(f\"Batch logits:\\n{batch_logits}\")\n",
    "    \n",
    "    # Test 3: Prediction methods\n",
    "    print(\"\\n3. Prediction methods:\")\n",
    "    predictions = model.predict(batch_texts)\n",
    "    print(f\"Probabilities:\\n{predictions}\")\n",
    "    \n",
    "    pred_with_conf = model.predict_with_confidence(batch_texts)\n",
    "    print(f\"Predictions: {pred_with_conf['predictions']}\")\n",
    "    print(f\"Confidence scores: {pred_with_conf['confidence']}\")\n",
    "    \n",
    "    print(\"\\n=== BACKWARD PASS TEST ===\")\n",
    "    \n",
    "    # Test 4: Backward pass (gradient computation)\n",
    "    model.train()\n",
    "    \n",
    "    # Create dummy targets\n",
    "    targets = torch.tensor([2, 0, 1])  # positive, negative, neutral\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = model.forward(batch_texts)\n",
    "    \n",
    "    # Compute loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(logits, targets)\n",
    "    \n",
    "    print(f\"\\n4. Backward pass:\")\n",
    "    print(f\"Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Check if gradients were computed\n",
    "    has_gradients = any(p.grad is not None for p in model.parameters())\n",
    "    print(f\"Gradients computed: {has_gradients}\")\n",
    "    \n",
    "    # Print a few gradient norms to verify\n",
    "    grad_norms = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            grad_norms.append((name, grad_norm))\n",
    "    \n",
    "    print(f\"\\nSample gradient norms:\")\n",
    "    for name, norm in grad_norms[:5]:  # Show first 5\n",
    "        print(f\"  {name}: {norm:.6f}\")\n",
    "    \n",
    "    print(f\"\\n✅ Model test completed successfully!\")\n",
    "    print(f\"✅ Forward pass: Working\")\n",
    "    print(f\"✅ Backward pass: Working\") \n",
    "    print(f\"✅ Ready for training!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csa (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
